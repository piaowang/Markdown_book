你可能就会问了，网页文件里面有那么多的无用信息，我们要怎么把它提取出来呢？

这个时候我们通常有两种做法，**一种是正则表达式，一种是通过网页的结构对内容提取。**



## bs

```python
from bs4 import BeautifulSoup
from urllib.request import urlopen

# if has Chinese, apply decode()
html =urlopen("https://morvanzhou.github.io/static/scraping/basic-structure.html").read().decode('utf-8')

print(html)
```



## 通用

总结一下： 在无法正确拿到数据时，请考虑以下因素：
1、selector是否写对？
2、网页源码中是否含有我要找的内容？
3、在headers中加入Agent-User或者cookie是否有用？
4、伪装成手机浏览器是否有用？
5、加入Referer是否有用？
6、其他

先长话短说 summarize 一下： 你需要学习： 

1. 基本的爬虫工作原理 

2. 基本的 http 抓取工具，Scrapy 

3. Bloom Filter:  http://billmill.org/bloomfilter-tutorial/  

4. 如果需要大规模网页抓取，你需要学习分布式爬虫的概念。其实没那么玄乎，你只要学会怎样维护一个所有集群机器能够有效分享的分布式队列就好。最简单的实现是 python-rq: https://github.com/nvie/rq   

5. rq 和 Scrapy 的结合：https://github.com/darkrho/scrapy-redis  

6. 后续处理，网页析取(https://github.com/grangier/python-goose )，存储(Mongodb) 

   ​

   ​

time.sleep()让爬虫睡眠间隔

- 第一：urllib的用法：

```
import urllib2  
response = urllib2.urlopen("http://www.baidu.com")
print response.read()
把一页HTML都打印出来，可以传递参数
```

- 第二：**设置 Headers**

有些网站不会同意程序直接用上面的方式进行访问，如果识别有问题，那么站点根本不会响应，所以为了完全模拟浏览器的工作，我们需要设置一些 Headers 的属性。

首先，打开我们的浏览器，调试浏览器 F12，我用的是 Chrome，打开网络监听，示意如下，比如知乎，点登录之后，我们会发现登陆之后界面都变化了，出现一个新的界面，实质上这个页面包含了许许多多的内容，这些内容也不是一次性就加载完成的，实质上是执行了好多次请求，一般是首先请求HTML文件，然后加载 JS，CSS 等等，经过多次请求之后，网页的骨架和肌肉全了，整个网页的效果也就出来了。

拆分这些请求，我们只看一第一个请求，你可以看到，有个 Request URL，还有 headers，下面便是 response，图片显示得不全，小伙伴们可以亲身实验一下。那么这个头中包含了许许多多是信息，有文件编码啦，压缩方式啦，请求的 agent 啦等等。

其中，agent 就是请求的身份，如果没有写入请求身份，那么服务器不一定会响应，所以可以在 headers 中设置agent,例如下面的例子，这个例子只是说明了怎样设置的 headers，小伙伴们看一下设置格式就好。

```
import urllib  
import urllib2  
url = 'http://www.server.com/login'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'  
values = {'username' : 'cqc',  'password' : 'XXXX' }  
headers = { 'User-Agent' : user_agent }  
data = urllib.urlencode(values)  
request = urllib2.Request(url, data, headers)  
response = urllib2.urlopen(request)  
page = response.read()   


headers = { 'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'  ,
                        'Referer':'http://www.zhihu.com/articles' }    
```

- 第三：

Cookie，指某些网站为了辨别用户身份、进行 session 跟踪而储存在用户本地终端上的数据（通常经过加密）

比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用 Urllib2 库保存我们登录的 Cookie，然后再抓取其他页面就达到目的了。

在此之前呢，我们必须先介绍一个 opener 的概念。

Opener

当你获取一个 URL 你使用一个 opener(一个 urllib2.OpenerDirector 的实例)。在前面，我们都是使用的默认的 opener，也就是 urlopen。它是一个特殊的 opener，可以理解成opener 的一个特殊实例，传入的参数仅仅是 url，data，timeout。

- 第四步

  正则表达式

  在前面我们已经搞定了怎样获取页面的内容，不过还差一步，这么多杂乱的代码夹杂文字我们怎样把它提取出来整理呢？下面就开始介绍一个十分强大的工具，正则表达式！

  ​

  了解正则表达式

  正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 正则表达式是用来匹配字符串非常强大的工具，在其他编程语言中同样有正则表达式的概念，Python同样不例外，利用了正则表达式，我们想要从返回的页面内容提取出我们想要的内容就易如反掌了。

  > 正则表达式的大致匹配过程是：

  1. 依次拿出表达式和文本中的字符比较，
  2. 如果每一个字符都能匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。
  3. 如果表达式中有量词或边界，这个过程会稍微有一些不同。

![img](http://wiki.jikexueyuan.com/project/python-crawler-guide/images/04.png)

 

## Python Re 模块

Python 自带了 re 模块，它提供了对正则表达式的支持。主要用到的方法列举如下

```
#返回pattern对象
re.compile(string[,flag])  
#以下为匹配所用函数
re.match(pattern, string[, flags])
re.search(pattern, string[, flags])
re.split(pattern, string[, maxsplit])
re.findall(pattern, string[, flags])
re.finditer(pattern, string[, flags])
re.sub(pattern, repl, string[, count])
re.subn(pattern, repl, string[, count])  
```

在介绍这几个方法之前，我们先来介绍一下 pattern 的概念，pattern 可以理解为一个匹配模式，那么我们怎么获得这个匹配模式呢？很简单，我们需要利用 re.compile 方法就可以。例如

```
pattern = re.compile(r'hello')  
```

在参数中我们传入了原生字符串对象，通过 compile 方法编译生成一个 pattern 对象，然后我们利用这个对象来进行进一步的匹配。

另外大家可能注意到了另一个参数 flags，在这里解释一下这个参数的含义：

参数 flag 是匹配模式，取值可以使用按位或运算符’|’表示同时生效，比如 re.I | re.M。



re.match(pattern, string[, flags])

这个方法将会从 string（我们要匹配的字符串）的开头开始，尝试匹配 pattern，一直向后匹配，如果遇到无法匹配的字符，立即返回 None，如果匹配未结束已经到达 string 的末尾，也会返回 None。两个结果均表示匹配失败，否则匹配 pattern 成功，同时匹配终止，不再对string 向后匹配。

## re

字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取 @ 前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。

正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。

在正则表达式中，如果直接给出字符，就是精确匹配。用 \d 可以匹配一个数字， \w 可以匹配一个字母或数字，所以：
'00\d' 可以匹配 '007' ，但无法匹配 '00A' ；
'\d\d\d' 可以匹配 '010' ；
'\w\w\d' 可以匹配 'py3' ；
**.**  可以匹配任意字符，所以：
'py.' 可以匹配 'pyc' 、 'pyo' 、 'py!' 等等



要匹配变长的字符，在正则表达式中，用 * 表示任意个字符（包括0个），用 + 表示至少一个字符，用 ? 表示0个或1个字符，用 {n} 表示n个字符，用 {n,m} 表示n-m个字符



**进阶** ：关键都是从左到右读取，限定只作用最近的左边

要做更精确地匹配，可以用 [] 表示范围，比如：
[0-9a-zA-Z **\   ** _ ] 可以匹配一个数字、字母或者下划线；
[0-9a-zA-Z**\ ** _ ]+ 可以匹配至少由一个数字、字母或者下划线组成的字符串，比如 'a100' ， '0_Z' ， 'Py3000' 等等；
**[**    a-zA-Z \ _   ][ 0-9a-zA-Z \ _]  * 可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；
**[**a-zA-Z\ _][0-9a-zA-Z\ _]{0, 19} 更精确地限制了变量的长度是1-20个字
符（前面1个字符+后面最多19个字符）。

A|B 可以匹配A或B，所以 [P|p]ython 可以匹配 'Python' 或者 'python' 。
^ 表示行的开头， ^\d 表示必须以数字开头。
**$ **   表示行的结束， \d$ 表示必须以数字结束。

你可能注意到了， py 也可以匹配 'python' ，但是加上 ^py$ 就变成了整行匹
配，就只能匹配 'py' 了

**切割字符串** 

```
切割空格
>>> re.split(r'\s+', 'a b  c')
['a', 'b', 'c']

切割了空格和逗号，加号表示至少一个
>>> re.split(r'[\s\,]+', 'a,b,d  c')
['a', 'b', 'd', 'c']

>>> re.split(r'[\s\,]+', 'a,b,d     c')
['a', 'b', 'd', 'c']
>>> re.split(r'[\s\,\;]+', 'a,b,d;;     c')
['a', 'b', 'd', 'c']
```

- **分组**

除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用 () 表示的就是要提取的分组（Group）。

^(\d{3})-(\d{3,8})$ 分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码

```
>>> m =re.match(r'^(\d{3})-(\d{3,8})$','010-123456')
>>> m
<_sre.SRE_Match object at 0x0000000002AE7F10>
>>> m.group(0)
'010-123456'
>>> m.group(1)
'010'
>>> m.group(2)
'123456'
```

如果正则表达式中定义了组，就可以在 Match 对象上用 group() 方法提取出子串来。



- 贪婪匹配
  最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。
  举例如下，匹配出数字后面的 0 

  ```
  >>> re.match(r'^(\d+)(0*)$', '102300').groups()
  ('102300', '')
  >>> re.match(r'^(\d+？)(0*)$', '102300').groups()
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
  AttributeError: 'NoneType' object has no attribute 'groups'

  非贪婪模式加了个？
  >>> re.match(r'^(\d+?)(0*)$', '102300').groups()
  ('1023', '00')
   
  >>> re.match(r'^(\d+?)$', '102300').groups()
  ('102300',)

  非贪婪模式
  >>> re.match(r'^(\d+?)', '102300').groups()
  ('1',)
  ```

```
  ​

  * 编译

  当我们在Python中使用正则表达式时，re模块内部会干两件事情：

  1. 编译正则表达式，如果正则表达式的字符串本身不合法，会报错；
  2. 用编译后的正则表达式去匹配字符串。

  如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则
  表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配：

```

> > > import re
> > >
> > > > > 编译

> > > re_telephone = re.compile(r'^(\d{3})-(\d{3,8})$')
> > >
> > > > > 使用：

> > > re_telephone.match('010-12345').groups()
> > >
> > > > > ('010', '12345')
> > > > > re_telephone.match('010-8086').groups()
> > > > > ('010', '8086')



